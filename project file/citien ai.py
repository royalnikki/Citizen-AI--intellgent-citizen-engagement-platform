# -*- coding: utf-8 -*-
"""citizen_ai_gradio_hf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B6X4QAM2_2JcsULXc-I5xNtTy5x_qn96
"""

!pip install transformers gradio --quiet

from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr

tokenizer = AutoTokenizer.from_pretrained("ibm-granite/granite-3.3-2b-instruct")
model = AutoModelForCausalLM.from_pretrained("ibm-granite/granite-3.3-2b-instruct")

def generate_response(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_length=150,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

iface = gr.Interface(
    fn=generate_response,
    inputs=[
        gr.Textbox(lines=5, placeholder="Describe your issue...", label="Citizen Query"),
        gr.File(label="Upload Photo (optional)")
    ],
    outputs=gr.Textbox(lines=10, label="AI Response"),
    title="Citizen AI Engagement Platform",
    description="Report issues or get info. Optionally upload images for better issue reporting."
)

iface = gr.Interface(
    fn=generate_response,
    inputs=gr.Textbox(lines=5, placeholder="Type your city service query or issue here...", label="Citizen Query"),
    outputs=gr.Textbox(lines=10, label="AI Response"),
    title="Citizen AI Engagement Platform",
    description="Submit city-related service queries, policies, or issues â€” get instant AI assistance."
)

iface.launch(share=True)

iface.launch(share=True)